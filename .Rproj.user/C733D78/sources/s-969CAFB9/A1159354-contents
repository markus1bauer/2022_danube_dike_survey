# This example shows how AIC selection, followed by a conventional regression analysis of the selected model, massively inflates false positives. CC BY-NC-SA 4.0 Florian Hartig 

set.seed(1)
library(MASS)

dat = data.frame(matrix(runif(20000), ncol = 100))
dat$y = rnorm(200)
fullModel = lm(y ~ . , data = dat)
summary(fullModel)
# 2 predictors out of 100 significant (on average, we expect 5 of 100 to be significant)

selection = stepAIC(fullModel)
summary(lm(y ~ X1 + X2 + X3 + X5 + X7 + X13 + X20 + X23 + X30 + 
             X37 + X42 + X45 + X46 + X47 + X48 + X64 + X65 + X66 + X71 + 
             X75 + X80 + X81 + X87 + X88 + X89 + X90 + X94 + X100, data = dat))

# voila, 15 out of 28 (before 100) predictors significant - looks like we could have good fun to discuss / publish these results!


#######################################################
# Same thing, but now we hide 10 significant predictors

set.seed(1)
library(MASS)

dat = data.frame(matrix(runif(20000), ncol = 100))
dat$y = rnorm(200)
dat$y = dat$y + rowSums(dat[,1:10]) 
fullModel = lm(y ~ . , data = dat)
summary(fullModel)
# 2 predictors out of 100 significant (on average, we expect 5 of 100 to be significant)

selection = stepAIC(fullModel)
summary(lm(y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X13 + 
             X14 + X20 + X23 + X24 + X26 + X30 + X37 + X42 + X46 + X47 + 
             X48 + X49 + X64 + X65 + X66 + X68 + X71 + X73 + X75 + X80 + 
             X81 + X87 + X88 + X89 + X90 + X94, data = dat))

# true positives are good, but false positives remain


# let's look at the same thing in JAGS, using various variable selection techniques

dat2 = scale(dat) # also response has to be scaled for adaptive shrinkage!
Data = list(y = dat2$y, x = as.matrix(dat2)[,1:100], i.max = nrow(dat2))

# fitting the full model - no problem for Jags
modelCode = "model{
  # Likelihood
  for(i in 1:i.max){
    mu[i] <- inprod(a , x[i,]) + b
    y[i] ~ dnorm(mu[i],tau)
  }
  
  # Prior distributions
  for(i in 1:100){
    a[i] ~ dnorm(0,0.01)
  }
  b ~ dnorm(0,0.01)

  tau ~ dgamma(0.001, 0.001)
  sigma <- 1/sqrt(tau)
}
"
jagsModel <- jags.model(file= textConnection(modelCode), data=Data, n.chains = 3)

para.names <- c("a","b","sigma")
Samples <- coda.samples(jagsModel, variable.names = para.names, n.iter = 5000)
gelman.diag(Samples)
summary(Samples)

par(mfrow = c(1,3))

x1<- summary(Samples)
res1 <- x1$quantiles[1:100,3] 
barplot(res1, names.arg = 1:100, las = 2)
abline(h = 1)
abline(v = 12)

# note: could implement fixed shrinkage via changing the precision of the normal distribution 


# adaptive shrinkage - idea is that we put a kind of "random effect" directly on the parameter values

modelCode = "model{

  # Likelihood
  for(i in 1:i.max){
    mu[i] <- inprod(a , x[i,]) + b
    y[i] ~ dnorm(mu[i],tau)
  }
  
  # Prior distributions
  for(i in 1:100){
    a[i] ~ dnorm(0,tauShrinkage)
  }
  b ~ dnorm(0,0.001)

  tauShrinkage ~ dgamma(0.001, 0.001)
  sdShrinkage <- 1/sqrt(tauShrinkage)
  
  tau ~ dgamma(0.001, 0.001)
  sigma <- 1/sqrt(tau)
}
"

# s = function(x) dgamma(x, shape = 0.0001, rate = 0.001)
# curve(s, 0, 5)

jagsModel <- jags.model(file= textConnection(modelCode), data=Data, n.chains = 3)

para.names <- c("a","b","sigma", "sdShrinkage")
Samples <- coda.samples(jagsModel, variable.names = para.names, n.iter = 5000)

gelman.diag(Samples)
summary(Samples)

x2<- summary(Samples)
res2 <- x2$quantiles[1:100,3] 
barplot(res2, names.arg = 1:100, las = 2)
abline(h = 1)
abline(v = 12)


#########################################################################

# spike and slab
# note - most of the time I see this combined with 

modelCode = "model{
  # Likelihood
  for(i in 1:i.max){
    mu[i] <- inprod(a , x[i,]) + b
    y[i] ~ dnorm(mu[i],tau)
  }

  # Prior distributions
  pind ~ dbeta(5,5)
  for(j in 1:100){
    a_raw[j] ~ dnorm(0,0.01)
    ind[j] ~ dbern(pind)
    a[j] = ind[j] * a_raw[j]
  }
  b ~ dnorm(0,0.01)
  
  tau ~ dgamma(0.001, 0.001)
  sigma <- 1/sqrt(tau)
  }
"


jagsModel <- jags.model(file= textConnection(modelCode), data=Data, n.chains = 3)

para.names <- c("a_raw", "ind","sigma")
Samples <- coda.samples(jagsModel, variable.names = para.names, n.iter = 5000)

summary(Samples)

gelman.diag(Samples)

marginalPlot(Samples, singlePanel = F, which = 101:116)


x3<- summary(Samples)
res3 <- x3$quantiles[101:200,3] 
barplot(res3, names.arg = 1:100, las = 2)
abline(h = 1)
abline(v = 12)



barplot(rbind(res1, res2, res3)[,1:30], beside = T)



# WAIC and Marginal Likelihoods can be calculated with the BayesianTools package!
